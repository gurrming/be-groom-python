import feedparser
import hashlib
import psycopg2
import time
from datetime import datetime, timezone

class CommunityBulkCollector:
    def __init__(self):
        self.db_config = {
            "host": "localhost", "port": "15432",
            "database": "app", "user": "postgres", "password": "0000"
        }
        # RSS ì£¼ì†Œ ë§¤í•‘
        self.subreddit_map = {
            "BTC": "bitcoin", "ETH": "ethereum", "SOL": "solana", "XRP": "xrp",
            "ADA": "cardano", "DOGE": "dogecoin", "DOT": "polkadot", "POL": "0xPolygon",
            "LINK": "chainlink", "TRX": "tronix", "LTC": "litecoin", "SHIB": "SHIBArmy",
            "AVAX": "Avax", "UNI": "Uniswap", "ATOM": "cosmosnetwork", "FIL": "filecoin"
        }

    def generate_hash(self, text):
        return hashlib.md5(text.encode()).hexdigest()

    def collect_bulk(self, ticker, sort_type):
        subreddit = self.subreddit_map.get(ticker, ticker.lower())
        min_date = datetime(2025, 10, 1, tzinfo=timezone.utc)
        split_date = datetime(2026, 1, 20, tzinfo=timezone.utc)

        conn = psycopg2.connect(**self.db_config)
        cur = conn.cursor()
        new_count = 0
        
        url = f"https://www.reddit.com/r/{subreddit}/{sort_type}/.rss"
        
        # User-Agent ì„¤ì • (í•„ìˆ˜)
        feed = feedparser.parse(url, agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
        
        # [ì¤‘ìš”] HTTP ìƒíƒœ ì½”ë“œ í™•ì¸ (ì°¨ë‹¨ ì—¬ë¶€ ì²´í¬)
        status = getattr(feed, 'status', 200)
        if status == 429:
            print(f"â›” [ì°¨ë‹¨ë¨-429] {ticker}-{sort_type}: ìš”ì²­ì´ ë„ˆë¬´ ë§ìŠµë‹ˆë‹¤. ì ì‹œ ëŒ€ê¸° í•„ìš”.")
            conn.close()
            return
        elif status != 200:
            print(f"âš ï¸ [ì ‘ì†ë¶ˆê°€-{status}] {ticker}-{sort_type}")
            conn.close()
            return

        if not feed.entries:
            # ìƒíƒœ ì½”ë“œëŠ” 200ì¸ë° ê¸€ì´ ì •ë§ ì—†ëŠ” ê²½ìš°
            print(f"   [ë¹ˆ ë°ì´í„°] {ticker}-{sort_type}: ê²Œì‹œê¸€ì´ ì—†ìŠµë‹ˆë‹¤.")
            conn.close()
            return

        for entry in feed.entries:
            title = entry.title
            description = entry.summary if 'summary' in entry else ""
            ups_count = int(entry.get('rank', 0))
            
            # ë‚ ì§œ íŒŒì‹± ì•ˆì „ì¥ì¹˜
            try:
                if hasattr(entry, 'updated_parsed'):
                    pub_date = datetime(*entry.updated_parsed[:6]).replace(tzinfo=timezone.utc)
                else:
                    pub_date = datetime.now(timezone.utc)
            except:
                continue
            
            if pub_date < min_date:
                continue

            is_test = pub_date >= split_date
            hash_key = self.generate_hash(f"{title}_{pub_date}")

            # [ìµœì¢… ìˆ˜ì •] 
            # 1. í…Œì´ë¸”ëª…: community_data
            # 2. ì»¬ëŸ¼ëª…: ticker -> symbol
            query = """
                INSERT INTO community_data
                (title, description, published_at, symbol, hash_key, platform, ups, is_test)
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
                ON CONFLICT (hash_key) 
                DO UPDATE SET ups = EXCLUDED.ups;
            """
            
            try:
                cur.execute(query, (
                    title, description, pub_date, ticker.upper(), 
                    hash_key, 'reddit', ups_count, is_test
                ))
                if cur.rowcount > 0:
                    new_count += 1
            except Exception as e:
                conn.rollback()
                # ì—ëŸ¬ ë°œìƒ ì‹œ ë¡œê·¸ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
                print(f"âŒ [DB Error] {ticker}: {e}")
                continue
        
        conn.commit()
        cur.close()
        conn.close()
        
        if new_count > 0:
            print(f"   âœ… [{ticker}-{sort_type}] {new_count}ê±´ ì €ì¥ ì™„ë£Œ")

if __name__ == "__main__":
    collector = CommunityBulkCollector()
    all_tickers = list(collector.subreddit_map.keys())
    
    while True:
        print(f"\nâ° [{datetime.now()}] RSS ìˆ˜ì§‘ ì‹œì‘ (ë”œë ˆì´ ì ìš©ë¨)...")
        
        for ticker in all_tickers:
            for s in ["new", "rising", "hot", "controversial"]:
                collector.collect_bulk(ticker, s)
                # [ì¤‘ìš”] ë ˆë”§ ì°¨ë‹¨ ë°©ì§€ë¥¼ ìœ„í•´ ë”œë ˆì´ë¥¼ 2ì´ˆë¡œ ëŠ˜ë¦¼
                time.sleep(2)
            
            # í•œ ì¢…ëª© ëë‚  ë•Œë§ˆë‹¤ 5ì´ˆ íœ´ì‹ (UNI, ATOM ë“± ë’·ìˆœì„œ ì°¨ë‹¨ ë°©ì§€)
            print(f"   ğŸ’¤ {ticker} ì™„ë£Œ. 5ì´ˆ ëŒ€ê¸°...")
            time.sleep(5)
            
        print("âœ¨ í•œ ë°”í€´ ì™„ë£Œ. 10ë¶„ ë’¤ ë‹¤ì‹œ ì‹œì‘í•©ë‹ˆë‹¤.")
        time.sleep(600)